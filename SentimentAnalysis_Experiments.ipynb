{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankbhushan/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string, re\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mitchellvii</td>\n",
       "      <td>2016-10-01 15:51</td>\n",
       "      <td>878</td>\n",
       "      <td>1216</td>\n",
       "      <td>\"Hillary attacked Trump for (allegedly) callin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"782352194473459713\"</td>\n",
       "      <td>https://twitter.com/mitchellvii/status/7823521...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noahsnab</td>\n",
       "      <td>2016-10-01 06:33</td>\n",
       "      <td>10938</td>\n",
       "      <td>20233</td>\n",
       "      <td>\"me: i hate michigan :(( detroit: polls zero p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"782211911723151360\"</td>\n",
       "      <td>https://twitter.com/noahsnab/status/7822119117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dt_ads</td>\n",
       "      <td>2016-10-01 12:20</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>\"If you feel US workers should have jobs befor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#AmericaFirst #dtmag</td>\n",
       "      <td>\"782299159571431424\"</td>\n",
       "      <td>https://twitter.com/dt_ads/status/782299159571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MikePenceVP</td>\n",
       "      <td>2016-10-01 13:14</td>\n",
       "      <td>471</td>\n",
       "      <td>415</td>\n",
       "      <td>\"Hillary called Trump Supporters \"deplorable\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#BasementDwellers</td>\n",
       "      <td>\"782312789968814080\"</td>\n",
       "      <td>https://twitter.com/MikePenceVP/status/7823127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JuddLegum</td>\n",
       "      <td>2016-10-01 08:18</td>\n",
       "      <td>641</td>\n",
       "      <td>1603</td>\n",
       "      <td>\"16. Trump claimed Google was involved in a co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"782238361046036480\"</td>\n",
       "      <td>https://twitter.com/JuddLegum/status/782238361...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username              date  retweets  favorites  \\\n",
       "0  mitchellvii  2016-10-01 15:51       878       1216   \n",
       "1     noahsnab  2016-10-01 06:33     10938      20233   \n",
       "2       dt_ads  2016-10-01 12:20        29         31   \n",
       "3  MikePenceVP  2016-10-01 13:14       471        415   \n",
       "4    JuddLegum  2016-10-01 08:18       641       1603   \n",
       "\n",
       "                                                text  geo mentions  \\\n",
       "0  \"Hillary attacked Trump for (allegedly) callin...  NaN      NaN   \n",
       "1  \"me: i hate michigan :(( detroit: polls zero p...  NaN      NaN   \n",
       "2  \"If you feel US workers should have jobs befor...  NaN      NaN   \n",
       "3  \"Hillary called Trump Supporters \"deplorable\" ...  NaN      NaN   \n",
       "4  \"16. Trump claimed Google was involved in a co...  NaN      NaN   \n",
       "\n",
       "               hashtags                    id  \\\n",
       "0                   NaN  \"782352194473459713\"   \n",
       "1                   NaN  \"782211911723151360\"   \n",
       "2  #AmericaFirst #dtmag  \"782299159571431424\"   \n",
       "3     #BasementDwellers  \"782312789968814080\"   \n",
       "4                   NaN  \"782238361046036480\"   \n",
       "\n",
       "                                           permalink  \n",
       "0  https://twitter.com/mitchellvii/status/7823521...  \n",
       "1  https://twitter.com/noahsnab/status/7822119117...  \n",
       "2  https://twitter.com/dt_ads/status/782299159571...  \n",
       "3  https://twitter.com/MikePenceVP/status/7823127...  \n",
       "4  https://twitter.com/JuddLegum/status/782238361...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/tweets.txt', sep = ';~;', engine='python')\n",
    "emoji_list = pd.read_csv('Data/emoji_table.txt', encoding='utf-8', index_col=0).index.values\n",
    "SentimentEmoji = pd.read_csv('Data/Emoji_classification.csv', encoding='utf-8').dropna()\n",
    "SentimentHashtags = pd.read_csv('Data/hashtags.csv', encoding='utf-8').dropna()\n",
    "hillaryTest = pd.read_csv('Hillary.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of positive and negative tweets\n",
    "sad = [':‑(', ':(', ':‑c', ':c', ':‑<', ':<', ':‑[' ,':[', ':-||', '>:[', ':{', ':@', '>:(']\n",
    "Positive = [':‑)',':)', ':-]', ':]',':-3', ':3', ':->', ':>' ,'8-)', '8)',':-}', ':}', ':o)', ':c)', ':^)' ,'=]', '=)'\n",
    "           ,':‑D', ':D', '8‑D', '8D', 'x‑D', 'xD', 'X‑D', 'XD', '=D', '=3', 'B^D']\n",
    "SentimentHashtags['HashtagSentiment'] = SentimentHashtags['HashtagSentiment'].map({'Positive':1, 'Negative':-1})\n",
    "SentimentEmoji['Sentiment'] = SentimentEmoji['Sentiment'].map({'Positive':1, 'Negative':-1, 'Neutral':0}).dropna()\n",
    "SentimentHashtags['Directed'] = SentimentHashtags['Directed'].map({'T':1, 'H':0})\n",
    "hillaryTest.Sentiment = hillaryTest.Sentiment.map({'Positive':1, 'Negative':-1, 'Neutral':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# removing for word2vec\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stop_list = stop_list + [\"rt\"] # Letting this remain so that rt are removed from the tweets\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# rt - stands for retweet\n",
    "\n",
    "\n",
    "# regex for capturing tweets\n",
    "reg = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "emoticons = \"|\".join(map(re.escape, sad + Positive))\n",
    "\n",
    "emoji_pattern = re.compile(u'('\n",
    "    u'\\ud83c[\\udf00-\\udfff]|'\n",
    "    u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "    u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "    re.UNICODE)\n",
    "classifier =[]\n",
    "# URL_Pat = re.compile(ur'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "def preprocess(tweet):\n",
    "    # only processing if the the value is a string\n",
    "    if type(tweet)!=type(2.0):\n",
    "        tweet = tweet.decode('latin-1').encode(\"utf-8\").decode('utf-8').strip()\n",
    "        tweet = tweet.lower()\n",
    "        # Removing hashtags\n",
    "        tweet = \" \".join(tweet.split('#'))\n",
    "        # Removing URLs\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https://[^\\s]+))','',tweet)\n",
    "        tweet = re.sub('((pic\\.[^\\s]+)|(https://[^\\s]+))','',tweet)\n",
    "        tweet = re.sub(u'[a-zA-Z0-9./]+\\.[a-zA-Z0-9./ ]+.*$','',tweet)\n",
    "        tweet = re.sub(\"(http\\S+)|(https\\S+)\", '', tweet)\n",
    "        # Removing User mentions\n",
    "        tweet = re.sub('@[^\\s]+','',tweet)\n",
    "        tweet = tweet.strip('\\'\"')\n",
    "        # Removing stop words - This can be moved to count vectorization\n",
    "        # tweet  = \" \".join([word for word in tweet.split(\" \") if word not in stop_list])\n",
    "        # lemmatizing words \n",
    "        tweet = \" \".join([lemmatizer.lemmatize(word) for word in tweet.split(\" \")])\n",
    "    else:\n",
    "        tweet=''\n",
    "    return tweet\n",
    "\n",
    "def extractEmoticons(tweet):\n",
    "    # emoji = emoji_pattern.findall(tweet)\n",
    "    emoji = []\n",
    "    for emo in emoji_list:\n",
    "        if emo in tweet:\n",
    "            emoji.append(emo)\n",
    "    \n",
    "    # these are :) :-) and other stuff\n",
    "    emoticons = re.findall(reg, tweet)\n",
    "    return \" , \".join(emoji + emoticons)\n",
    "def removeEmoticons(tweet):\n",
    "    return re.sub(reg,'',tweet)\n",
    "\n",
    "# data = data.dropna()\n",
    "data['processed_text'] = data.text.apply(preprocess)\n",
    "hillaryTest['processed_text'] = hillaryTest.processed_text.apply(preprocess)\n",
    "\n",
    "#getting the emoticons from the cleaned data\n",
    "data['emoticons'] = data['processed_text'].apply(extractEmoticons)\n",
    "\n",
    "# Removing emoticons from the text data\n",
    "data['processed_text'] = data['processed_text'].apply(removeEmoticons)\n",
    "print 'Completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankbhushan/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "HillaryTweets = data[data['processed_text'].str.contains('((hil.?ary)|(clinton))', case = False)]\n",
    "DonaldTweets = data[data['processed_text'].str.contains('trump', case = False)]\n",
    "\n",
    "datasets = [HillaryTweets.copy(), DonaldTweets.copy()]\n",
    "TrainSets = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    hashtags = datasets[i]['hashtags'].copy().str.split(' ').apply(pd.Series, 1).stack()\n",
    "    hashtags.index = hashtags.index.droplevel(-1)\n",
    "    datasets[i].drop('hashtags', axis=1, inplace=True)\n",
    "    hashtags.name = 'hashtags'\n",
    "    \n",
    "    datasets[i] = datasets[i].join(hashtags.str.strip())\n",
    "    \n",
    "    emoticons = datasets[i]['emoticons'].copy().str.split(' ').apply(pd.Series, 1).stack()\n",
    "    emoticons.index = emoticons.index.droplevel(-1)\n",
    "    datasets[i].drop('emoticons', axis=1, inplace=True)\n",
    "    emoticons.name = 'emoticons'\n",
    "    datasets[i] = datasets[i].join(emoticons.str.strip())\n",
    "    \n",
    "    Directed_hashtags = SentimentHashtags[SentimentHashtags['Directed'] == 0]\n",
    "    Opp_hashtags = SentimentHashtags[SentimentHashtags['Directed'] != 0]\n",
    "    Opp_hashtags.loc[: ,'HashtagSentiment'] = Opp_hashtags.HashtagSentiment * -1;\n",
    "    \n",
    "    Directed_hashtags = Directed_hashtags.append(Opp_hashtags)\n",
    "    datasets[i] = pd.merge(datasets[i], Directed_hashtags, on = 'hashtags', how='outer')\n",
    "    datasets[i] = pd.merge(datasets[i], SentimentEmoji, on = 'emoticons', how='outer')\n",
    "    datasets[i]['Sentiment'] = datasets[i]['HashtagSentiment'].add(datasets[i]['Sentiment'], fill_value = 0)\n",
    "    TrainSets.append(datasets[i][['username', 'date', 'processed_text', 'Sentiment']].dropna().groupby(['processed_text', 'Sentiment']).max().reset_index())\n",
    "\n",
    "data_train = datasets[0][['processed_text','Sentiment']].copy().dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_list)\n",
    "#vectorizer = HashingVectorizer(stop_words=stop_list)\n",
    "#vectorizer = TfidfVectorizer(stop_words=stop_list)\n",
    "\n",
    "X = vectorizer.fit_transform(data_train.processed_text.append(hillaryTest.processed_text))\n",
    "X_train = X[0:data_train.processed_text.shape[0]]\n",
    "Y_train = data_train['Sentiment']\n",
    "#X_test = vectorizer.fit(hillaryTest.processed_text)\n",
    "\n",
    "#model = RandomForestClassifier()\n",
    "#model.fit(X_train, Y_train)\n",
    "\n",
    "#preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 72\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "tot = 0\n",
    "for i, pred in enumerate(preds):\n",
    "    if(hillaryTest.Sentiment[i] == hillaryTest.Sentiment[i]):\n",
    "        tot+=1\n",
    "        if(hillaryTest.Sentiment[i] == pred):\n",
    "            score+=1\n",
    "print score, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=3, n_init=1,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=True)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 26488.945\n",
      "Iteration  1, inertia 13590.235\n",
      "Iteration  2, inertia 13573.981\n",
      "Iteration  3, inertia 13558.029\n",
      "Iteration  4, inertia 13547.756\n",
      "Iteration  5, inertia 13539.660\n",
      "Iteration  6, inertia 13538.935\n",
      "Iteration  7, inertia 13538.329\n",
      "Iteration  8, inertia 13538.170\n",
      "Iteration  9, inertia 13538.093\n",
      "Iteration 10, inertia 13538.071\n",
      "Iteration 11, inertia 13538.060\n",
      "Iteration 12, inertia 13538.049\n",
      "Iteration 13, inertia 13538.031\n",
      "Iteration 14, inertia 13538.024\n",
      "Iteration 15, inertia 13538.015\n",
      "Iteration 16, inertia 13537.991\n",
      "Iteration 17, inertia 13537.987\n",
      "Iteration 18, inertia 13537.980\n",
      "Iteration 19, inertia 13537.969\n",
      "Iteration 20, inertia 13537.963\n",
      "Iteration 21, inertia 13537.953\n",
      "Iteration 22, inertia 13537.946\n",
      "Iteration 23, inertia 13537.920\n",
      "Iteration 24, inertia 13537.906\n",
      "Iteration 25, inertia 13537.894\n",
      "Iteration 26, inertia 13537.887\n",
      "Converged at iteration 26\n",
      "done in 1.079s\n",
      "()\n",
      "Cluster 0:\n",
      " pledged\n",
      " pledge\n",
      " pleasing\n",
      " pleasegodno\n",
      " pleasefollowback\n",
      " pleasantly\n",
      " pleasant\n",
      " plead\n",
      " plaza\n",
      " 00\n",
      "()\n",
      "Cluster 1:\n",
      " poodles\n",
      " poodle\n",
      " poochie\n",
      " pony\n",
      " pontificating\n",
      " pontificate\n",
      " polo\n",
      " polltrolling\n",
      " pollster\n",
      " 00\n",
      "()\n",
      "Cluster 2:\n",
      " thedoctor\n",
      " imbecile\n",
      " hypocrites\n",
      " pac\n",
      " paraphrase\n",
      " partic\n",
      " hopefuls\n",
      " soup\n",
      " drezner\n",
      " tonigh\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=True)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(3):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, -10:]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def get_words(tweet):\n",
    "    return tweet.split(' ')\n",
    "tweets = pd.Series(data['processed_text'].unique()).apply(get_words)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 140    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(tweets, workers=num_workers, size=num_features, min_count = min_word_count, window = context,\n",
    "                          sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"30features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
