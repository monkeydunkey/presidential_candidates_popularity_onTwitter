{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string, re\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/tweets.txt', sep = ';~;', engine='python')\n",
    "PositiveTweets = pd.read_csv('Data/tweetsPositive.txt', sep = ';~;', engine='python')\n",
    "NegativeTweets = pd.read_csv('Data/tweetsNegative.txt', sep = ';~;', engine='python')\n",
    "\n",
    "emoji_list = pd.read_csv('Data/emoji_table.txt', encoding='utf-8', index_col=0).index.values\n",
    "SentimentEmoji = pd.read_csv('Data/Emoji_classification.csv', encoding='utf-8').dropna()\n",
    "SentimentHashtags = pd.read_csv('Data/hashtags.csv', encoding='utf-8').dropna()\n",
    "\n",
    "## The test set for hillary\n",
    "hillaryTest = pd.read_csv('Hillary.csv')\n",
    "TrumpTest = pd.read_csv('Trump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of positive and negative tweets\n",
    "sad = [':‑(', ':(', ':‑c', ':c', ':‑<', ':<', ':‑[' ,':[', ':-||', '>:[', ':{', ':@', '>:(']\n",
    "Positive = [':‑)',':)', ':-]', ':]',':-3', ':3', ':->', ':>' ,'8-)', '8)',':-}', ':}', ':o)', ':c)', ':^)' ,'=]', '=)'\n",
    "           ,':‑D', ':D', '8‑D', '8D', 'x‑D', 'xD', 'X‑D', 'XD', '=D', '=3', 'B^D']\n",
    "SentimentHashtags['HashtagSentiment'] = SentimentHashtags['HashtagSentiment'].map({'Positive':1, 'Negative':-1})\n",
    "SentimentEmoji['Sentiment'] = SentimentEmoji['Sentiment'].map({'Positive':1, 'Negative':-1, 'Neutral':0}).dropna()\n",
    "SentimentHashtags['Directed'] = SentimentHashtags['Directed'].map({'T':1, 'H':0})\n",
    "hillaryTest.Sentiment = hillaryTest.Sentiment.map({'Positive':1, 'Negative':-1, 'Neutral':0})\n",
    "TrumpTest.Sentiment = TrumpTest.Sentiment.map({'Positive':1, 'Negative':-1, 'Neutral':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "stop_list = nltk.corpus.stopwords.words('english') + [\"rt\"] # rt - stands for retweet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# regex for capturing tweets\n",
    "reg = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "emoticons = \"|\".join(map(re.escape, sad + Positive))\n",
    "\n",
    "emoji_pattern = re.compile(u'('\n",
    "    u'\\ud83c[\\udf00-\\udfff]|'\n",
    "    u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "    u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "    re.UNICODE)\n",
    "classifier =[]\n",
    "def preprocess(tweet):\n",
    "    # only processing if the the value is a string\n",
    "    if type(tweet)!=type(2.0):\n",
    "        tweet = tweet.decode('latin-1').encode(\"utf-8\").decode('utf-8').strip()\n",
    "        tweet = tweet.lower()\n",
    "        # Removing hashtags\n",
    "        tweet = \" \".join(tweet.split('#'))\n",
    "        # Removing URLs\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https://[^\\s]+))','',tweet)\n",
    "        tweet = re.sub('((pic\\.[^\\s]+)|(https://[^\\s]+))','',tweet)\n",
    "        tweet = re.sub(\"(http\\S+)|(https\\S+)\", '', tweet)\n",
    "        # Adding this pattern to the last cause it will remove everything after the start of a URL\n",
    "        tweet = re.sub(u'[a-zA-Z0-9./]+\\.[a-zA-Z0-9./ ]+.*$','',tweet)\n",
    "        \n",
    "        # Removing User mentions\n",
    "        tweet = re.sub('@[^\\s]+','',tweet)\n",
    "        tweet = tweet.strip('\\'\"')\n",
    "        # Removing stop words - This can be moved to count vectorization\n",
    "        # tweet  = \" \".join([word for word in tweet.split(\" \") if word not in stop_list])\n",
    "        # lemmatizing words \n",
    "        tweet = \" \".join([lemmatizer.lemmatize(word) for word in tweet.split(\" \")])\n",
    "    else:\n",
    "        tweet=''\n",
    "    return tweet\n",
    "\n",
    "def extractEmoticons(tweet):\n",
    "    # emoji = emoji_pattern.findall(tweet)\n",
    "    emoji = []\n",
    "    for emo in emoji_list:\n",
    "        if emo in tweet:\n",
    "            emoji.append(emo)\n",
    "    \n",
    "    # these are :) :-) and other stuff\n",
    "    emoticons = re.findall(reg, tweet)\n",
    "    return \" , \".join(emoji + emoticons)\n",
    "def removeEmoticons(tweet):\n",
    "    return re.sub(reg,'',tweet)\n",
    "\n",
    "#Processing the tweets\n",
    "data['processed_text'] = data.text.apply(preprocess)\n",
    "hillaryTest['processed_text'] = hillaryTest.processed_text.apply(preprocess)\n",
    "TrumpTest['processed_text'] = TrumpTest.processed_text.apply(preprocess)\n",
    "PositiveTweets['processed_text'] = PositiveTweets.text.apply(preprocess)\n",
    "NegativeTweets['processed_text'] = NegativeTweets.text.apply(preprocess)\n",
    "\n",
    "\n",
    "#getting the emoticons from the cleaned data\n",
    "data['emoticons'] = data['processed_text'].apply(extractEmoticons)\n",
    "\n",
    "# Removing emoticons from the text data\n",
    "data['processed_text'] = data['processed_text'].apply(removeEmoticons)\n",
    "\n",
    "data = data.append(PositiveTweets).append(NegativeTweets)\n",
    "print 'Completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankbhushan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n",
      "/Users/shashankbhushan/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "HillaryTweets = data[data['processed_text'].str.contains('((hil.?ary)|(clinton))', case = False)]\n",
    "DonaldTweets = data[data['processed_text'].str.contains('trump', case = False)]\n",
    "\n",
    "datasets = [HillaryTweets.copy(), DonaldTweets.copy()]\n",
    "TrainSets = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    hashtags = datasets[i]['hashtags'].copy().str.split(' ').apply(pd.Series, 1).stack()\n",
    "    hashtags.index = hashtags.index.droplevel(-1)\n",
    "    datasets[i].drop('hashtags', axis=1, inplace=True)\n",
    "    hashtags.name = 'hashtags'\n",
    "    \n",
    "    datasets[i] = datasets[i].join(hashtags.str.strip())\n",
    "    \n",
    "    emoticons = datasets[i]['emoticons'].copy().str.split(' ').apply(pd.Series, 1).stack()\n",
    "    emoticons.index = emoticons.index.droplevel(-1)\n",
    "    datasets[i].drop('emoticons', axis=1, inplace=True)\n",
    "    emoticons.name = 'emoticons'\n",
    "    datasets[i] = datasets[i].join(emoticons.str.strip())\n",
    "    \n",
    "    Directed_hashtags = SentimentHashtags[SentimentHashtags['Directed'] == 0]\n",
    "    Opp_hashtags = SentimentHashtags[SentimentHashtags['Directed'] != 0]\n",
    "    Opp_hashtags.loc[: ,'HashtagSentiment'] = Opp_hashtags.HashtagSentiment * -1;\n",
    "    \n",
    "    Directed_hashtags = Directed_hashtags.append(Opp_hashtags)\n",
    "    datasets[i] = pd.merge(datasets[i], Directed_hashtags, on = 'hashtags', how='outer')\n",
    "    datasets[i] = pd.merge(datasets[i], SentimentEmoji, on = 'emoticons', how='outer')\n",
    "    datasets[i]['Sentiment'] = datasets[i]['HashtagSentiment'].add(datasets[i]['Sentiment'], fill_value = 0)\n",
    "    TrainSets.append(datasets[i][['username', 'date', 'processed_text', 'Sentiment']].dropna().groupby(['processed_text', 'Sentiment']).max().reset_index())\n",
    "\n",
    "data_train = TrainSets[0][['processed_text','Sentiment']].copy().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "The score for Vectorizer: Count , Model: RandomForest 45 72\n",
      "0 1\n",
      "The score for Vectorizer: Count , Model: Gradient Boosting 44 72\n",
      "0 2\n",
      "The score for Vectorizer: Count , Model: XGB Classifier 22 72\n",
      "1 0\n",
      "The score for Vectorizer: Hash , Model: RandomForest 45 72\n",
      "1 1\n",
      "The score for Vectorizer: Hash , Model: Gradient Boosting 45 72\n",
      "1 2\n",
      "The score for Vectorizer: Hash , Model: XGB Classifier 22 72\n",
      "2 0\n",
      "The score for Vectorizer: TF-IDF , Model: RandomForest 45 72\n",
      "2 1\n",
      "The score for Vectorizer: TF-IDF , Model: Gradient Boosting 44 72\n",
      "2 2\n",
      "The score for Vectorizer: TF-IDF , Model: XGB Classifier 22 72\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [CountVectorizer(stop_words=stop_list), HashingVectorizer(stop_words=stop_list), TfidfVectorizer(stop_words=stop_list)]\n",
    "vectorizersName = ['Count', 'Hash', 'TF-IDF']\n",
    "for k, vectorizer in enumerate(vectorizers):\n",
    "    X = vectorizer.fit_transform(data_train.processed_text.append(TrumpTest.processed_text))\n",
    "    X_train = X[0:data_train.processed_text.shape[0]]\n",
    "    Y_train = data_train['Sentiment']\n",
    "    X_test = X[data_train.processed_text.shape[0]:]\n",
    "    models = [RandomForestClassifier(), GradientBoostingClassifier(), XGBClassifier()]\n",
    "    modelsName = ['RandomForest', 'Gradient Boosting', 'XGB Classifier']\n",
    "    for m, model in enumerate(models):\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test.todense())\n",
    "        score = 0\n",
    "        tot = 0\n",
    "        for i, pred in enumerate(preds):\n",
    "            if(hillaryTest.Sentiment[i] == hillaryTest.Sentiment[i]):\n",
    "                tot+=1\n",
    "                if(hillaryTest.Sentiment[i] == pred):\n",
    "                    score+=1\n",
    "\n",
    "        print 'The score for Vectorizer:', vectorizersName[k],', Model:', modelsName[m], score, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for Vectorizer: TF-IDF , Model: Random Forest 31 84\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_list)\n",
    "X = vectorizer.fit_transform(data_train.processed_text.append(TrumpTest.processed_text))\n",
    "X_train = X[0:data_train.processed_text.shape[0]]\n",
    "Y_train = data_train['Sentiment']\n",
    "X_test = X[data_train.processed_text.shape[0]:]\n",
    "models = [RandomForestClassifier(n_estimators = 10)]\n",
    "for m, model in enumerate(models):\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test.todense())\n",
    "        score = 0\n",
    "        tot = 0\n",
    "        for i, pred in enumerate(preds):\n",
    "            if(TrumpTest.Sentiment[i] == TrumpTest.Sentiment[i]):\n",
    "                tot+=1\n",
    "                if(TrumpTest.Sentiment[i] == pred):\n",
    "                    score+=1\n",
    "\n",
    "        print 'The score for Vectorizer:', vectorizersName[k],', Model: Random Forest', score, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def get_words(tweet):\n",
    "    return tweet.split(' ')\n",
    "tweets = pd.Series(data['processed_text'].unique()).apply(get_words)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 140    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(tweets, workers=num_workers, size=num_features, min_count = min_word_count, window = context,\n",
    "                          sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"30features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = TrainSets[1][['processed_text','Sentiment']].copy().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrumpTest['processed_text'] = TrumpTest.processed_text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
